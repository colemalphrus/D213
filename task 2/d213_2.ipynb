{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# D213 Task 1 Advanced Data Analytics\n",
    "\n",
    "## Part 1\n",
    "\n",
    "### A1: Research Question and Data Selection\n",
    "\n",
    "Research Question:\n",
    "Is it possible to at accurately determine customer sentiment from a customers review utilizing Natural language processing and nural networks?\n",
    "\n",
    "Data and Rational:\n",
    "The data that will be used for this analysis is \"sentiment labeled sentences\" dataset which can be found at the link bellow. This data set provides a sentence representing a review along with a label of 1 or 0 indicating a positive or negative sentiment respectively\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences\n",
    "\n",
    "### A2: Objectives\n",
    "\n",
    "The objective of this analysis is to determine the feasibility of using a natural language processing neural network to determine a customers sentiment based on a review. The determination of feasibility will be made by creating a NLP model using Tensorflow. The objective of this model is to be able to take in the review text data and determine if the review has a positive or negative sentiment. \n",
    "\n",
    "### A3: Neural Network Type\n",
    "\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a432f0908da7590"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2\n",
    "\n",
    "### B1: Data Exploration and Cleaning\n",
    "\n",
    "1. Check presence of unusual characters\n",
    "2. Vocabulary size\n",
    "3. proposed word embedding length\n",
    "4. statistical justification for the chosen maximum sequence length\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d804fcb64e734f34"
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'trapezoid' from 'sklearn.utils.fixes' (/Users/orlandmalphrus/.local/share/virtualenvs/D213-JXc5fzL4/lib/python3.9/site-packages/sklearn/utils/fixes.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[209], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stopwords\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tokenizer\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/D213-JXc5fzL4/lib/python3.9/site-packages/sklearn/model_selection/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_plot\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LearningCurveDisplay, ValidationCurveDisplay\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_search\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GridSearchCV, ParameterGrid, ParameterSampler, RandomizedSearchCV\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_split\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      6\u001B[0m     BaseCrossValidator,\n\u001B[1;32m      7\u001B[0m     BaseShuffleSplit,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     train_test_split,\n\u001B[1;32m     25\u001B[0m )\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/D213-JXc5fzL4/lib/python3.9/site-packages/sklearn/model_selection/_plot.py:7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_matplotlib_support\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_plotting\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _interval_max_min_ratio, _validate_score_name\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m learning_curve, validation_curve\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01m_BaseCurveDisplay\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_plot_curve\u001B[39m(\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     13\u001B[0m         x_data,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m         errorbar_kw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     24\u001B[0m     ):\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/D213-JXc5fzL4/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m clone, is_classifier\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FitFailedWarning\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_scoring, get_scorer_names\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_scorer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _check_multimetric_scoring, _MultimetricScorer\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LabelEncoder\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/D213-JXc5fzL4/lib/python3.9/site-packages/sklearn/metrics/__init__.py:31\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dist_metrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DistanceMetric\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_plot\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfusion_matrix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConfusionMatrixDisplay\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_plot\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdet_curve\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DetCurveDisplay\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_plot\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprecision_recall_curve\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PrecisionRecallDisplay\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_plot\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregression\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PredictionErrorDisplay\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/D213-JXc5fzL4/lib/python3.9/site-packages/sklearn/metrics/_plot/det_curve.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_plotting\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _BinaryClassifierCurveDisplayMixin\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ranking\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m det_curve\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mDetCurveDisplay\u001B[39;00m(_BinaryClassifierCurveDisplayMixin):\n\u001B[1;32m      8\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"DET curve visualization.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    It is recommend to use :func:`~sklearn.metrics.DetCurveDisplay.from_estimator`\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;124;03m    >>> plt.show()\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/D213-JXc5fzL4/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:41\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_param_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Interval, StrOptions, validate_params\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextmath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stable_cumsum\n\u001B[0;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfixes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trapezoid\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmulticlass\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m type_of_target\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparsefuncs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m count_nonzero\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'trapezoid' from 'sklearn.utils.fixes' (/Users/orlandmalphrus/.local/share/virtualenvs/D213-JXc5fzL4/lib/python3.9/site-packages/sklearn/utils/fixes.py)"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download Stopwords early to avoid rerunning\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:56:45.298373Z",
     "start_time": "2023-09-25T01:56:45.103858Z"
    }
   },
   "id": "5abee81299e7824a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract data create combined dataframe and check counts\n",
    "amazon_df = pd.read_csv('./data/amazon_cells_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "imdb_df = pd.read_csv('./data/imdb_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "yelp_df = pd.read_csv('./data/yelp_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "\n",
    "print(f'Amazon Count: {amazon_df.shape[0]}')\n",
    "print(f'IMDB Count: {imdb_df.shape[0]}')\n",
    "print(f'Yelp Count: {yelp_df.shape[0]}')\n",
    "\n",
    "# Label Data Source\n",
    "amazon_df['source'] = 'amz'\n",
    "imdb_df['source'] = 'imdb'\n",
    "yelp_df['source'] = 'yelp'\n",
    "\n",
    "# Join Dataframes \n",
    "df = pd.concat([amazon_df, imdb_df, yelp_df], ignore_index=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.298201Z"
    }
   },
   "id": "6129db4b8935b16a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:56:45.306435Z",
     "start_time": "2023-09-25T01:56:45.300075Z"
    }
   },
   "id": "2bb5843d8a8d76fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get all Unique chars from the dataset \n",
    "# Convert all reviews to a single string and then to a set to get unique characters\n",
    "unique_chars = set(''.join(df['review']))\n",
    "print('All Unique Characters:')\n",
    "print(unique_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.301454Z"
    }
   },
   "id": "499675515bf4478c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_alpha_numeric_chars = [char for char in unique_chars if not char.isalnum()]\n",
    "print('Non alpha numeric characters:')\n",
    "print(non_alpha_numeric_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.302859Z"
    }
   },
   "id": "a20cef73165584a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove non-alphanumeric chars\n",
    "df['cleaned_review'] = df['review'].apply(lambda x: ''.join([char for char in x if char.isalnum() or char.isspace()]))\n",
    "# Remove stopwords\n",
    "df['cleaned_reduced_review'] = df['cleaned_review'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.303892Z"
    }
   },
   "id": "ba493c94854dff61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Vocabulary Size\n",
    "all_words = ' '.join(df['cleaned_reduced_review']).lower().split()\n",
    "vocabulary = set(all_words)\n",
    "vocabulary_size =  len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.304937Z"
    }
   },
   "id": "530926871244443c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Proposed word embedding length. Based on an industry rule of thumb for embeddings taking a forth root (Goldman, 2019)\n",
    "proposed_embedding_length = round(vocabulary_size ** .25)\n",
    "print(f\"Proposed embedding length: {proposed_embedding_length}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T01:56:45.306700Z",
     "start_time": "2023-09-25T01:56:45.306601Z"
    }
   },
   "id": "54e323678f716359"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Justification of max sequence length\n",
    "df['cleaned_review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "clean_review_length_mean = df['cleaned_review_length'].mean()\n",
    "clean_review_length_std = df['cleaned_review_length'].std()\n",
    "clean_review_length_max = df['cleaned_review_length'].max()\n",
    "\n",
    "print(f\"Mean clean_review_length: {clean_review_length_mean}\")\n",
    "print(f\"Standard deviation of clean_review_length: {clean_review_length_std}\")\n",
    "print(f\"Maximum clean_review_length: {clean_review_length_max}\")\n",
    "\n",
    "# max length that covers around 95% of the dataset\n",
    "cutoff_length = int(clean_review_length_mean + 2 * clean_review_length_std)\n",
    "print(f\"Suggested max sequence length: {cutoff_length}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.307872Z"
    }
   },
   "id": "1a5bf26dffcdeeaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test train split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.308800Z"
    }
   },
   "id": "176ac1e539b6e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B2: Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f43cc954a0476f6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='OOV')\n",
    "tokenizer.fit_on_texts(train_df['cleaned_reduced_review'])\n",
    "train_tokens = tokenizer.texts_to_sequences(train_df['cleaned_reduced_review'])\n",
    "test_tokens = tokenizer.texts_to_sequences(test_df['cleaned_reduced_review'])\n",
    "\n",
    "test_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.309694Z"
    }
   },
   "id": "f07924fa8721ecef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T01:56:45.310761Z"
    }
   },
   "id": "435e2a848701072"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
