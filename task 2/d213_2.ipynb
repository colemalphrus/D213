{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# D213 Task 1 Advanced Data Analytics\n",
    "\n",
    "## Part 1\n",
    "\n",
    "### A1: Research Question and Data Selection\n",
    "\n",
    "Research Question:\n",
    "Is it possible to at accurately determine customer sentiment from a customers review utilizing Natural language processing and nural networks?\n",
    "\n",
    "Data and Rational:\n",
    "The data that will be used for this analysis is \"sentiment labeled sentences\" dataset which can be found at the link bellow. This data set provides a sentence representing a review along with a label of 1 or 0 indicating a positive or negative sentiment respectively\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences\n",
    "\n",
    "### A2: Objectives\n",
    "\n",
    "The objective of this analysis is to determine the feasibility of using a natural language processing neural network to determine a customers sentiment based on a review. The determination of feasibility will be made by creating a NLP model using Tensorflow. The objective of this model is to be able to take in the review text data and determine if the review has a positive or negative sentiment. \n",
    "\n",
    "### A3: Neural Network Type\n",
    "\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a432f0908da7590"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2\n",
    "\n",
    "### B1: Data Exploration and Cleaning\n",
    "\n",
    "1. Check presence of unusual characters\n",
    "2. Vocabulary size\n",
    "3. proposed word embedding length\n",
    "4. statistical justification for the chosen maximum sequence length\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d804fcb64e734f34"
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/orlandmalphrus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Download Stopwords early to avoid rerunning\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.811190Z",
     "start_time": "2023-09-25T05:19:52.698421Z"
    }
   },
   "id": "5abee81299e7824a"
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Count: 1000\n",
      "IMDB Count: 1000\n",
      "Yelp Count: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                              review  score source\n0  So there is no way for me to plug it in here i...      0    amz\n1                        Good case, Excellent value.      1    amz\n2                             Great for the jawbone.      1    amz\n3  Tied to charger for conversations lasting more...      0    amz\n4                                  The mic is great.      1    amz",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>score</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So there is no way for me to plug it in here i...</td>\n      <td>0</td>\n      <td>amz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good case, Excellent value.</td>\n      <td>1</td>\n      <td>amz</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Great for the jawbone.</td>\n      <td>1</td>\n      <td>amz</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tied to charger for conversations lasting more...</td>\n      <td>0</td>\n      <td>amz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The mic is great.</td>\n      <td>1</td>\n      <td>amz</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data create combined dataframe and check counts\n",
    "amazon_df = pd.read_csv('./data/amazon_cells_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "imdb_df = pd.read_csv('./data/imdb_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "yelp_df = pd.read_csv('./data/yelp_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "\n",
    "print(f'Amazon Count: {amazon_df.shape[0]}')\n",
    "print(f'IMDB Count: {imdb_df.shape[0]}')\n",
    "print(f'Yelp Count: {yelp_df.shape[0]}')\n",
    "\n",
    "# Label Data Source\n",
    "amazon_df['source'] = 'amz'\n",
    "imdb_df['source'] = 'imdb'\n",
    "yelp_df['source'] = 'yelp'\n",
    "\n",
    "# Join Dataframes \n",
    "df = pd.concat([amazon_df, imdb_df, yelp_df], ignore_index=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.813445Z",
     "start_time": "2023-09-25T05:19:52.706388Z"
    }
   },
   "id": "6129db4b8935b16a"
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  3000 non-null   object\n",
      " 1   score   3000 non-null   int64 \n",
      " 2   source  3000 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 70.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.813554Z",
     "start_time": "2023-09-25T05:19:52.728730Z"
    }
   },
   "id": "2bb5843d8a8d76fd"
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Unique Characters:\n",
      "{'å', 'Z', 'E', 'c', 'o', 'C', '(', 'U', 'Q', 'X', 'V', 'Y', '-', 'a', '1', 'K', 'G', 'b', \"'\", 'M', 'N', 'f', 's', 'm', 'y', ':', '.', ',', 'S', 'j', '6', 'r', 'u', '*', '2', 'R', 'z', 'w', 'I', 'L', 'B', 'F', '\\x97', 'k', 'T', 'W', '\\x85', 'n', 'H', 'D', ']', '5', 'x', 'i', 'd', '8', 'e', ')', 'P', 'J', '/', '3', 't', '\\x96', 'é', 'h', 'g', '$', ';', 'l', 'q', '!', 'O', '7', '+', '0', '\"', '?', '[', ' ', '%', '9', '#', '4', 'v', 'ê', 'p', 'A', '&'}\n"
     ]
    }
   ],
   "source": [
    "# Get all Unique chars from the dataset \n",
    "# Convert all reviews to a single string and then to a set to get unique characters\n",
    "unique_chars = set(''.join(df['review']))\n",
    "print('All Unique Characters:')\n",
    "print(unique_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.815436Z",
     "start_time": "2023-09-25T05:19:52.733766Z"
    }
   },
   "id": "499675515bf4478c"
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non alpha numeric characters:\n",
      "['(', '-', \"'\", ':', '.', ',', '*', '\\x97', '\\x85', ']', ')', '/', '\\x96', '$', ';', '!', '+', '\"', '?', '[', ' ', '%', '#', '&']\n"
     ]
    }
   ],
   "source": [
    "non_alpha_numeric_chars = [char for char in unique_chars if not char.isalnum()]\n",
    "print('Non alpha numeric characters:')\n",
    "print(non_alpha_numeric_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.815517Z",
     "start_time": "2023-09-25T05:19:52.736902Z"
    }
   },
   "id": "a20cef73165584a7"
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review  score source  \\\n0  So there is no way for me to plug it in here i...      0    amz   \n1                        Good case, Excellent value.      1    amz   \n2                             Great for the jawbone.      1    amz   \n3  Tied to charger for conversations lasting more...      0    amz   \n4                                  The mic is great.      1    amz   \n\n                                      cleaned_review  \\\n0  So there is no way for me to plug it in here i...   \n1                          Good case Excellent value   \n2                              Great for the jawbone   \n3  Tied to charger for conversations lasting more...   \n4                                   The mic is great   \n\n                              cleaned_reduced_review  \n0                    way plug us unless go converter  \n1                          good case excellent value  \n2                                      great jawbone  \n3  tied charger conversations lasting 45 minutesm...  \n4                                          mic great  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>score</th>\n      <th>source</th>\n      <th>cleaned_review</th>\n      <th>cleaned_reduced_review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So there is no way for me to plug it in here i...</td>\n      <td>0</td>\n      <td>amz</td>\n      <td>So there is no way for me to plug it in here i...</td>\n      <td>way plug us unless go converter</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good case, Excellent value.</td>\n      <td>1</td>\n      <td>amz</td>\n      <td>Good case Excellent value</td>\n      <td>good case excellent value</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Great for the jawbone.</td>\n      <td>1</td>\n      <td>amz</td>\n      <td>Great for the jawbone</td>\n      <td>great jawbone</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tied to charger for conversations lasting more...</td>\n      <td>0</td>\n      <td>amz</td>\n      <td>Tied to charger for conversations lasting more...</td>\n      <td>tied charger conversations lasting 45 minutesm...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The mic is great.</td>\n      <td>1</td>\n      <td>amz</td>\n      <td>The mic is great</td>\n      <td>mic great</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove non-alphanumeric chars\n",
    "df['cleaned_review'] = df['review'].apply(lambda x: ''.join([char for char in x if char.isalnum() or char.isspace()]))\n",
    "# Remove stopwords\n",
    "df['cleaned_reduced_review'] = df['cleaned_review'].apply(lambda x: ' '.join([word.lower() for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.815995Z",
     "start_time": "2023-09-25T05:19:52.746817Z"
    }
   },
   "id": "ba493c94854dff61"
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5276\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Size\n",
    "all_words = ' '.join(df['cleaned_reduced_review']).lower().split()\n",
    "vocabulary = set(all_words)\n",
    "vocabulary_size =  len(vocabulary)\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.816300Z",
     "start_time": "2023-09-25T05:19:52.784831Z"
    }
   },
   "id": "530926871244443c"
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed embedding length: 9\n"
     ]
    }
   ],
   "source": [
    "# Proposed word embedding length. Based on an industry rule of thumb for embeddings taking a forth root (Goldman, 2019)\n",
    "proposed_embedding_length = round(vocabulary_size ** .25)\n",
    "print(f\"Proposed embedding length: {proposed_embedding_length}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.816435Z",
     "start_time": "2023-09-25T05:19:52.785016Z"
    }
   },
   "id": "54e323678f716359"
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean clean_review_length: 11.777666666666667\n",
      "Standard deviation of clean_review_length: 7.8309221893934255\n",
      "Maximum clean_review_length: 70\n",
      "Suggested max sequence length: 27\n"
     ]
    }
   ],
   "source": [
    "# Justification of max sequence length\n",
    "df['cleaned_review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "clean_review_length_mean = df['cleaned_review_length'].mean()\n",
    "clean_review_length_std = df['cleaned_review_length'].std()\n",
    "clean_review_length_max = df['cleaned_review_length'].max()\n",
    "\n",
    "print(f\"Mean clean_review_length: {clean_review_length_mean}\")\n",
    "print(f\"Standard deviation of clean_review_length: {clean_review_length_std}\")\n",
    "print(f\"Maximum clean_review_length: {clean_review_length_max}\")\n",
    "\n",
    "# max length that covers around 95% of the dataset\n",
    "cutoff_length = int(clean_review_length_mean + 2 * clean_review_length_std)\n",
    "print(f\"Suggested max sequence length: {cutoff_length}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.816492Z",
     "start_time": "2023-09-25T05:19:52.792583Z"
    }
   },
   "id": "1a5bf26dffcdeeaa"
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "outputs": [],
   "source": [
    "# Test train split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.816567Z",
     "start_time": "2023-09-25T05:19:52.797116Z"
    }
   },
   "id": "176ac1e539b6e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B2: Tokenization\n",
    "\n",
    "The goal of tokenization is to break down words or groups of words into numerical representations that are easily consumed by machines. The tokenizer this analysis employees is from the tensorflow keras library. The output of the tokenization process is an array of integers that can be used for further analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f43cc954a0476f6b"
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='OOV')\n",
    "tokenizer.fit_on_texts(train_df['cleaned_reduced_review'])\n",
    "train_df['tokens'] = tokenizer.texts_to_sequences(train_df['cleaned_reduced_review'])\n",
    "test_df['tokens'] = tokenizer.texts_to_sequences(test_df['cleaned_reduced_review'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.892028Z",
     "start_time": "2023-09-25T05:19:52.821641Z"
    }
   },
   "id": "f07924fa8721ecef"
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 cleaned_reduced_review  \\\n2369                                   deserves 5 stars   \n1164  good thing tickets cost five dollars would mad...   \n477     volume switch rocketed unit destination unknown   \n1728                   nice relaxing late night viewing   \n1065  one character totally annoying voice gives fee...   \n...                                                 ...   \n2514                           everything perfect night   \n2347  classywarm atmosphere fun fresh appetizers suc...   \n1608                         accents absolutely abysmal   \n2541  waited thirty minutes seated although 8 vacant...   \n2575              pizza tasted old super chewy good way   \n\n                                                 tokens  score  \n2369                                    [617, 180, 181]      1  \n1164  [2, 56, 1791, 618, 619, 1119, 16, 1792, 233, 6...      0  \n477                  [259, 1794, 1795, 328, 1796, 1797]      0  \n1728                        [30, 1798, 1120, 142, 1121]      1  \n1065      [7, 126, 114, 508, 329, 621, 203, 1799, 1800]      0  \n...                                                 ...    ...  \n2514                                     [67, 198, 142]      1  \n2347  [4612, 374, 273, 153, 1726, 4613, 738, 4614, 169]      1  \n1608                                  [4615, 137, 4616]      0  \n2541  [365, 4617, 83, 759, 394, 611, 4618, 598, 629,...      0  \n2575                   [191, 410, 173, 357, 860, 2, 44]      0  \n\n[2400 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cleaned_reduced_review</th>\n      <th>tokens</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2369</th>\n      <td>deserves 5 stars</td>\n      <td>[617, 180, 181]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1164</th>\n      <td>good thing tickets cost five dollars would mad...</td>\n      <td>[2, 56, 1791, 618, 619, 1119, 16, 1792, 233, 6...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>477</th>\n      <td>volume switch rocketed unit destination unknown</td>\n      <td>[259, 1794, 1795, 328, 1796, 1797]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1728</th>\n      <td>nice relaxing late night viewing</td>\n      <td>[30, 1798, 1120, 142, 1121]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1065</th>\n      <td>one character totally annoying voice gives fee...</td>\n      <td>[7, 126, 114, 508, 329, 621, 203, 1799, 1800]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2514</th>\n      <td>everything perfect night</td>\n      <td>[67, 198, 142]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2347</th>\n      <td>classywarm atmosphere fun fresh appetizers suc...</td>\n      <td>[4612, 374, 273, 153, 1726, 4613, 738, 4614, 169]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1608</th>\n      <td>accents absolutely abysmal</td>\n      <td>[4615, 137, 4616]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2541</th>\n      <td>waited thirty minutes seated although 8 vacant...</td>\n      <td>[365, 4617, 83, 759, 394, 611, 4618, 598, 629,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2575</th>\n      <td>pizza tasted old super chewy good way</td>\n      <td>[191, 410, 173, 357, 860, 2, 44]</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2400 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['cleaned_reduced_review' ,'tokens', 'score']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.899755Z",
     "start_time": "2023-09-25T05:19:52.848040Z"
    }
   },
   "id": "435e2a848701072"
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 cleaned_reduced_review  \\\n1291                                        rating 1 10   \n595                              possesed get junk idea   \n236                               best bluetooth market   \n2824                                      awful service   \n1290                                     camerawork bad   \n...                                                 ...   \n2236  descriptions said yum yum sauce another said e...   \n1448                              810 score mostly plot   \n1189                           excellent performance ms   \n8                             needless say wasted money   \n310                        good battery got really fast   \n\n                                                 tokens  score  \n1291                                    [625, 225, 132]      0  \n595                                  [1, 35, 562, 1012]      0  \n236                                      [22, 229, 898]      1  \n2824                                          [149, 13]      0  \n1290                                         [2909, 14]      0  \n...                                                 ...    ...  \n2236  [1, 212, 1, 1, 420, 154, 212, 1, 420, 352, 154...      0  \n1448                                 [1, 4472, 510, 87]      1  \n1189                                    [39, 204, 4321]      1  \n8                                   [1266, 58, 409, 89]      0  \n310                                [2, 47, 51, 12, 235]      1  \n\n[600 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cleaned_reduced_review</th>\n      <th>tokens</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1291</th>\n      <td>rating 1 10</td>\n      <td>[625, 225, 132]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>possesed get junk idea</td>\n      <td>[1, 35, 562, 1012]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>best bluetooth market</td>\n      <td>[22, 229, 898]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2824</th>\n      <td>awful service</td>\n      <td>[149, 13]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1290</th>\n      <td>camerawork bad</td>\n      <td>[2909, 14]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2236</th>\n      <td>descriptions said yum yum sauce another said e...</td>\n      <td>[1, 212, 1, 1, 420, 154, 212, 1, 420, 352, 154...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1448</th>\n      <td>810 score mostly plot</td>\n      <td>[1, 4472, 510, 87]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1189</th>\n      <td>excellent performance ms</td>\n      <td>[39, 204, 4321]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>needless say wasted money</td>\n      <td>[1266, 58, 409, 89]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>310</th>\n      <td>good battery got really fast</td>\n      <td>[2, 47, 51, 12, 235]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>600 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[['cleaned_reduced_review' ,'tokens', 'score']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.928850Z",
     "start_time": "2023-09-25T05:19:52.856243Z"
    }
   },
   "id": "a2a9230d2f2500f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B3: Padding Process\n",
    "\n",
    "The padding process takes advantage of the Keras `pad_sequences` function. This function takes in an array of tokens and applies padding. Padding is necessary to ensure the machine has inputs of the same shape(length). By setting `padding` and `truncating` to 'post' the function will ensure that padding or truncation is applied to the end of the sequence. By setting `maxlen` to the cutoff established earlier in this analysis (27) the function will truncate records to this length. This introduces a serious tradeoff between the potential loss of information and computational efficiency. With this in mind this analysis uses a cutoff that is equal to two times the standard deviation sequence length. This should ensure that 95% the data is preserved while avoiding excessive padding."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "346c7d01bb430435"
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff: 27\n"
     ]
    }
   ],
   "source": [
    "print(f'Cutoff: {cutoff_length}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.929283Z",
     "start_time": "2023-09-25T05:19:52.861823Z"
    }
   },
   "id": "38706f51c94ccac1"
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          padded_tokens              tokens  \\\n1291  [625, 225, 132, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [625, 225, 132]   \n595   [1, 35, 562, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [1, 35, 562, 1012]   \n236   [22, 229, 898, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...      [22, 229, 898]   \n2824  [149, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           [149, 13]   \n1290  [2909, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...          [2909, 14]   \n\n      score  \n1291      0  \n595       0  \n236       1  \n2824      0  \n1290      0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>padded_tokens</th>\n      <th>tokens</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1291</th>\n      <td>[625, 225, 132, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[625, 225, 132]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>[1, 35, 562, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 35, 562, 1012]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>[22, 229, 898, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n      <td>[22, 229, 898]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2824</th>\n      <td>[149, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[149, 13]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1290</th>\n      <td>[2909, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n      <td>[2909, 14]</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['padded_tokens'] = list(pad_sequences(train_df['tokens'].tolist(), padding='post', maxlen=cutoff_length, truncating='post'))\n",
    "test_df['padded_tokens'] = list(pad_sequences(test_df['tokens'].tolist(), padding='post', maxlen=cutoff_length, truncating='post'))\n",
    "\n",
    "test_df[['padded_tokens' ,'tokens', 'score']].head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.958304Z",
     "start_time": "2023-09-25T05:19:52.866664Z"
    }
   },
   "id": "cdf92106fd3fc237"
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          padded_tokens  \\\n2369  [617, 180, 181, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1164  [2, 56, 1791, 618, 619, 1119, 16, 1792, 233, 6...   \n477   [259, 1794, 1795, 328, 1796, 1797, 0, 0, 0, 0,...   \n1728  [30, 1798, 1120, 142, 1121, 0, 0, 0, 0, 0, 0, ...   \n1065  [7, 126, 114, 508, 329, 621, 203, 1799, 1800, ...   \n\n                                                 tokens  score  \n2369                                    [617, 180, 181]      1  \n1164  [2, 56, 1791, 618, 619, 1119, 16, 1792, 233, 6...      0  \n477                  [259, 1794, 1795, 328, 1796, 1797]      0  \n1728                        [30, 1798, 1120, 142, 1121]      1  \n1065      [7, 126, 114, 508, 329, 621, 203, 1799, 1800]      0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>padded_tokens</th>\n      <th>tokens</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2369</th>\n      <td>[617, 180, 181, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[617, 180, 181]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1164</th>\n      <td>[2, 56, 1791, 618, 619, 1119, 16, 1792, 233, 6...</td>\n      <td>[2, 56, 1791, 618, 619, 1119, 16, 1792, 233, 6...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>477</th>\n      <td>[259, 1794, 1795, 328, 1796, 1797, 0, 0, 0, 0,...</td>\n      <td>[259, 1794, 1795, 328, 1796, 1797]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1728</th>\n      <td>[30, 1798, 1120, 142, 1121, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[30, 1798, 1120, 142, 1121]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1065</th>\n      <td>[7, 126, 114, 508, 329, 621, 203, 1799, 1800, ...</td>\n      <td>[7, 126, 114, 508, 329, 621, 203, 1799, 1800]</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['padded_tokens' ,'tokens', 'score']].head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.958595Z",
     "start_time": "2023-09-25T05:19:52.881295Z"
    }
   },
   "id": "882bb8c2aa8d3272"
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 301, 98, 446, 25, 3566, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Printing one example of a padded token array\n",
    "\n",
    "print(train_df['padded_tokens'][0].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:52.958676Z",
     "start_time": "2023-09-25T05:19:52.884460Z"
    }
   },
   "id": "1fe279bca4ca1e56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B4: Categories of Sentiment\n",
    "\n",
    "There are two categories of sentiment in this analysis `positive` represented by 1 and `negative` represented by 0. Due to the binary nature of target categories a sigmoid activation function will be used in this analysis. The sigmoid function will output a value between 0 and 1 which will represent the probability of a review is positive and therefore the inverse will show the probability the review is negative. \n",
    "\n",
    "### B5: Data Preparation Steps\n",
    "\n",
    "1. **Loading of Tools and Stopwords**: At the beginning of this analysis we load all needed tools. Loading of stopwords is included in tool import as it contains time-consuming network activity.\n",
    "2.  **Data loading**: The yelp, imdb and amazon data is loaded from tab separated files. These three datasets provide the reviews for the sentiment analysis.\n",
    "3. **Join Data Sources and Review Counts**: To ensure the data loaded properly counts are examined for each of the three datasets. After confirming that the counts match the datasets are joined into one main dataframe `df`.\n",
    "4. **Data Summary**: Data info summery is produced using pandas dataframe builtin info method.\n",
    "5. **Handling of unusual characters and punctuation**: A set is produced containing all unique characters within the dataset. The presence of non-standard characters was confirmed by visualizing the character set. Non-standard characters were removed along with all punctuation by only retaining alphanumeric characters. This reduces potential complexity, and potential noise from the analysis.\n",
    "6. **Removal of Stopwords**: Stopwords are removed using NLTK's set of words that are so commonly used that there presence is unlikely to add value to the analysis.\n",
    "7. **Vocabulary Size Determination**:  The vocabulary size is determined by taking the count of unique words present in the cleaned reduced review set.\n",
    "8. **Word Embedding Length Determination**: The word embedding length was determined using an industry rule of thumb by taking a forth root of the vocabulary size(Goldman, 2019).\n",
    "9. **Max sequence length Determination**: The max sequence length was determined by using statistical measures to produce a cutoff that would preserve 95% of the dataset while still providing a reasonable cutoff.\n",
    "10. **Splitting test and train data**: The dataset was then split into a training data set that consisted of 80% of the records and a test set consisting of the remaining 20%. Sklearn provided the train_test_split function that was used in this process.\n",
    "11. **Tokenization**: Tokenization was applied using a Keras tokenizer. This transformed the textual review data to an array of integers representing the text data.\n",
    "12. **Padding**: Padding was added to the token array to produce token arrays of the same length. A more detailed review of this process is available in section B3.\n",
    "13. **Data Export** In the following the full dataset will be exported to CSV along with the test and train clean dataframes. This will be done using pandas builtin to_csv method.\n",
    "\n",
    "\n",
    "### B6: Cleaned Data Set\n",
    "\n",
    "Attached to submission"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a166dad6d3684002"
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [],
   "source": [
    "df.to_csv('./full_data_set.csv')\n",
    "test_df.to_csv('./test_data_set.csv')\n",
    "train_df.to_csv('./train_data_set.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:53.078735Z",
     "start_time": "2023-09-25T05:19:52.887714Z"
    }
   },
   "id": "261a9874ed4a830c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3: Network Architecture\n",
    "\n",
    "### C1: Output of Model Summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b511b37e606bb1ec"
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=proposed_embedding_length, input_length = cutoff_length, mask_zero=True))\n",
    "model.add(SimpleRNN(64, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:53.110103Z",
     "start_time": "2023-09-25T05:19:53.029717Z"
    }
   },
   "id": "3bfc7c0a9914f27d"
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_18 (Embedding)    (None, 27, 9)             47484     \n",
      "                                                                 \n",
      " simple_rnn_19 (SimpleRNN)   (None, 64)                4736      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52285 (204.24 KB)\n",
      "Trainable params: 52285 (204.24 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:53.110813Z",
     "start_time": "2023-09-25T05:19:53.080275Z"
    }
   },
   "id": "911222965139c724"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### C2: Layer Descriptions\n",
    "\n",
    "This model contains three layers an embedding layer, a simple RNN layer and a Dense output layer. The total number of parameters is 52,285 all of which are trainable. A breakdown of the layers and their parameters is bellow.\n",
    "\n",
    "**Embedding Layer**: The embedding layer serves to convert the integer encoded textual data into dense vectors that the remainder of the network can consume. It does this taking into consideration the previously determined cutoff value, vocabulary length and embedding dimensionality. This layer accounts for 47,484 parameters.\n",
    "\n",
    "**RNN Layer (Simple RNN)**  SimpleRNN is a basic recurrent neural network which allows for the capture patterns in the order of data in the review sequence. This layer accounts for 4736 parameters.\n",
    "\n",
    "**Dense Layer (output / activation)**: The final layer in the model is a dense layer that applies a sigmoid activation function. This layer is the output layer. This layer accounts for 65 parameters.\n",
    "\n",
    "\n",
    "### C3: Hyperparameter Justification\n",
    "\n",
    "**Activation function**: Due to the binary nature of this classification this analysis will use a sigmoid function as the activation layer. The sigmoid function will return a value between one and zero which will represent the probability that the sentiment is positive. \n",
    "\n",
    "**Nodes per Layer**:\n",
    "- Embedding dimension: the embedding layer does not contain a specified number of nodes but represents the total vocabulary as 9 dimensional vectors.\n",
    "- RNN Layer: The RNN layer has 64 nodes. Traditional start values in this type of analysis are 32, 64, or 128. 64 was chosen here to compensate for the low dimensionality within the embedding layer as it is larger than the base of 32. 64 was a better start than 128 as the scope of the vocabulary was not large enough to demand that level of compensation. The model preformed well with 64 nodes so this value was not modified.\n",
    "- Dense Layer (Activation): The final layer contains 1 node. Due to the binary nature of this problem a single node is appropriate for the output layer. \n",
    "\n",
    "**Loss Function**: Since this sentiment analysis is binary in nature with either a positive or negative sentiment Binary Crossentropy is the appropriate choice. Binary Crossentropy measures distance between the label and the predicted value which is ideal for this analysis.\n",
    "\n",
    "**Optimizer**: Adam is used in this analysis as it is a highly adaptive optimization algorithm. Adam is often applied as default optimizer and due to the strait forward nature of this analysis there is no current need to find a more specialized solution.\n",
    "\n",
    "**Stopping Criteria**: EarlyStopping is used in this analysis to prevent under/over fitting of the model. Early Stopping allows the model to stop evaluating when performance degrades. This saves computational resources and training time.\n",
    "\n",
    "**Evaluation Metric**: The binary nature of this analysis makes Accuracy the ideal metric to evaluate this model on. Accuracy is an easily interpretable metric that is easily translatable to business value. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "785955be6b104219"
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:53.110929Z",
     "start_time": "2023-09-25T05:19:53.088289Z"
    }
   },
   "id": "c794e730ac41e8f9"
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:53.110974Z",
     "start_time": "2023-09-25T05:19:53.089678Z"
    }
   },
   "id": "fd71e5a70331fdc6"
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "outputs": [],
   "source": [
    "X_train = np.array([x.flatten() for x in train_df['padded_tokens'].tolist()])\n",
    "y_train = train_df['score'].values\n",
    "X_test = np.array([x.flatten() for x in test_df['padded_tokens'].tolist()])\n",
    "y_test = test_df['score'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:53.155321Z",
     "start_time": "2023-09-25T05:19:53.092107Z"
    }
   },
   "id": "9da300f502fcce72"
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 617  180  181 ...    0    0    0]\n",
      " [   2   56 1791 ...    0    0    0]\n",
      " [ 259 1794 1795 ...    0    0    0]\n",
      " ...\n",
      " [4615  137 4616 ...    0    0    0]\n",
      " [ 365 4617   83 ...    0    0    0]\n",
      " [ 191  410  173 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(X_train )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:53.161491Z",
     "start_time": "2023-09-25T05:19:53.102772Z"
    }
   },
   "id": "b9ddec65bb6035e7"
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5700 - val_loss: 0.6536 - val_accuracy: 0.6450\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.8225 - val_loss: 0.5491 - val_accuracy: 0.7250\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2468 - accuracy: 0.9254 - val_loss: 0.5555 - val_accuracy: 0.7700\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1355 - accuracy: 0.9600 - val_loss: 0.5611 - val_accuracy: 0.7783\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.9808 - val_loss: 0.7094 - val_accuracy: 0.7817\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.0581 - accuracy: 0.9871 - val_loss: 0.7728 - val_accuracy: 0.7833\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 0.9912 - val_loss: 0.7838 - val_accuracy: 0.7850\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.9921 - val_loss: 0.8355 - val_accuracy: 0.7783\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.9942 - val_loss: 0.9223 - val_accuracy: 0.7850\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.9942 - val_loss: 0.9587 - val_accuracy: 0.7883\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.9587 - accuracy: 0.7883\n",
      "Accuracy: 78.83%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:56.247625Z",
     "start_time": "2023-09-25T05:19:53.107975Z"
    }
   },
   "id": "9a178bd7d756f472"
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:56.252466Z",
     "start_time": "2023-09-25T05:19:56.246210Z"
    }
   },
   "id": "a34e0ff92e5ff69d"
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T05:19:56.252762Z",
     "start_time": "2023-09-25T05:19:56.249096Z"
    }
   },
   "id": "f73362b8d5942888"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
