{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# D213 Task 1 Advanced Data Analytics\n",
    "\n",
    "## Part 1\n",
    "\n",
    "### A1: Research Question and Data Selection\n",
    "\n",
    "Research Question:\n",
    "Is it possible to at accurately determine customer sentiment from a customers review utilizing Natural language processing and nural networks?\n",
    "\n",
    "Data and Rational:\n",
    "The data that will be used for this analysis is \"sentiment labeled sentences\" dataset which can be found at the link bellow. This data set provides a sentence representing a review along with a label of 1 or 0 indicating a positive or negative sentiment respectively\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences\n",
    "\n",
    "### A2: Objectives\n",
    "\n",
    "The objective of this analysis is to determine the feasibility of using a natural language processing neural network to determine a customers sentiment based on a review. The determination of feasibility will be made by creating a NLP model using Tensorflow. The objective of this model is to be able to take in the review text data and determine if the review has a positive or negative sentiment. \n",
    "\n",
    "### A3: Neural Network Type\n",
    "\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a432f0908da7590"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2\n",
    "\n",
    "### B1: Data Exploration and Cleaning\n",
    "\n",
    "1. Check presence of unusual characters\n",
    "2. Vocabulary size\n",
    "3. proposed word embedding length\n",
    "4. statistical justification for the chosen maximum sequence length\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d804fcb64e734f34"
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[157], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stopwords\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download Stopwords early to avoid rerunning\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:57:07.436896Z",
     "start_time": "2023-09-24T22:57:07.424616Z"
    }
   },
   "id": "5abee81299e7824a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract data create combined dataframe and check counts\n",
    "amazon_df = pd.read_csv('./data/amazon_cells_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "imdb_df = pd.read_csv('./data/imdb_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "yelp_df = pd.read_csv('./data/yelp_labelled.txt', sep='\\t', names=['review', 'score'])\n",
    "\n",
    "print(f'Amazon Count: {amazon_df.shape[0]}')\n",
    "print(f'IMDB Count: {imdb_df.shape[0]}')\n",
    "print(f'Yelp Count: {yelp_df.shape[0]}')\n",
    "\n",
    "# Label Data Source\n",
    "amazon_df['source'] = 'amz'\n",
    "imdb_df['source'] = 'imdb'\n",
    "yelp_df['source'] = 'yelp'\n",
    "\n",
    "# Join Dataframes \n",
    "df = pd.concat([amazon_df, imdb_df, yelp_df], ignore_index=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:56:57.202562Z",
     "start_time": "2023-09-24T22:56:57.196650Z"
    }
   },
   "id": "6129db4b8935b16a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T22:56:57.198178Z"
    }
   },
   "id": "2bb5843d8a8d76fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get all Unique chars from the dataset \n",
    "# Convert all reviews to a single string and then to a set to get unique characters\n",
    "unique_chars = set(''.join(df['review']))\n",
    "print('All Unique Characters:')\n",
    "print(unique_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T22:56:57.202237Z"
    }
   },
   "id": "499675515bf4478c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_alpha_numeric_chars = [char for char in unique_chars if not char.isalnum()]\n",
    "print('Non alpha numeric characters:')\n",
    "print(non_alpha_numeric_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:56:57.204787Z",
     "start_time": "2023-09-24T22:56:57.204442Z"
    }
   },
   "id": "a20cef73165584a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['cleaned_review'] = df['review'].apply(lambda x: ''.join([char for char in x if char.isalnum() or char.isspace()]))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-24T22:56:57.206030Z"
    }
   },
   "id": "ba493c94854dff61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Vocabulary Size\n",
    "all_words = ' '.join(df['cleaned_review']).lower().split()\n",
    "vocab_size = len(set(all_words))\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:56:57.208940Z",
     "start_time": "2023-09-24T22:56:57.207517Z"
    }
   },
   "id": "530926871244443c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Justification of max sequence length\n",
    "df['cleaned_review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "clean_review_length_mean = df['cleaned_review_length'].mean()\n",
    "clean_review_length_std = df['cleaned_review_length'].std()\n",
    "clean_review_length_max = df['cleaned_review_length'].max()\n",
    "\n",
    "print(f\"Mean clean_review_length: {clean_review_length_mean}\")\n",
    "print(f\"Standard deviation of clean_review_length: {clean_review_length_std}\")\n",
    "print(f\"Maximum clean_review_length: {clean_review_length_max}\")\n",
    "\n",
    "# max length that covers around 95% of the dataset\n",
    "cutoff_length = int(clean_review_length_mean + 2 * clean_review_length_std)\n",
    "print(f\"Suggested max sequence length: {cutoff_length}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:56:57.209538Z",
     "start_time": "2023-09-24T22:56:57.209307Z"
    }
   },
   "id": "1a5bf26dffcdeeaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T22:56:57.211396Z",
     "start_time": "2023-09-24T22:56:57.210183Z"
    }
   },
   "id": "25b9b9c3d98d6c0b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
